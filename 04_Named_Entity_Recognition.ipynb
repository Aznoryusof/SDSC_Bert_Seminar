{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-Named_Entity_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOA3KZvDgzTL5t4H24qH4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demoleiwang/SDSC_Bert_Seminar/blob/master/04_Named_Entity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAMY65g4RCal",
        "colab_type": "text"
      },
      "source": [
        "# A Simple Example for Named Entity Recognition with Bert\n",
        "\n",
        "If you have any questions, feel free to contact us\n",
        "\n",
        "We reference a lot from this link: https://github.com/IINemo/bert_sequence_tagger/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxPZeYDSSMBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5233407-0c13-4b52-9e03-898d319c787f"
      },
      "source": [
        "!pip install transformers flair seqeval\n",
        "\n",
        "!mkdir -p conll2003\n",
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa -O ./conll2003/eng.testa\n",
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb -O ./conll2003/eng.testb\n",
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train -O ./conll2003/eng.train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.10)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.7.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.14.37)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.17.37)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n",
            "--2020-08-16 08:05:14--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827012 (808K) [text/plain]\n",
            "Saving to: ‘./conll2003/eng.testa’\n",
            "\n",
            "./conll2003/eng.tes 100%[===================>] 807.63K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-08-16 08:05:14 (11.7 MB/s) - ‘./conll2003/eng.testa’ saved [827012/827012]\n",
            "\n",
            "--2020-08-16 08:05:16--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748096 (731K) [text/plain]\n",
            "Saving to: ‘./conll2003/eng.testb’\n",
            "\n",
            "./conll2003/eng.tes 100%[===================>] 730.56K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-08-16 08:05:16 (11.2 MB/s) - ‘./conll2003/eng.testb’ saved [748096/748096]\n",
            "\n",
            "--2020-08-16 08:05:18--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3281528 (3.1M) [text/plain]\n",
            "Saving to: ‘./conll2003/eng.train’\n",
            "\n",
            "./conll2003/eng.tra 100%[===================>]   3.13M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-08-16 08:05:18 (25.3 MB/s) - ‘./conll2003/eng.train’ saved [3281528/3281528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcPbvppfRvwT",
        "colab_type": "text"
      },
      "source": [
        "## Basic configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2Apj3jW7eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18616556-2413-4601-9b3b-4a1bd2aefd7a"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger('sequence_tagger_bert')\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# torch.cuda.set_device(2)\n",
        "print (\"device:\", device)\n",
        "print(torch.cuda.get_device_name())\n",
        "\n",
        "torch.manual_seed(2020)\n",
        "\n",
        "CACHE_DIR = 'cache'\n",
        "BATCH_SIZE = 16\n",
        "PRED_BATCH_SIZE = 100\n",
        "MAX_LEN = 128\n",
        "MAX_N_EPOCHS = 2\n",
        "WEIGHT_DECAY = 0.01\n",
        "LEARNING_RATE = 5e-5"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDDQFQ2lR-wF",
        "colab_type": "text"
      },
      "source": [
        "## Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8gZPne1RySg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "4d3070cd-1140-4fee-feeb-d499c33800d9"
      },
      "source": [
        "\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "\n",
        "data_folder = 'conll2003'\n",
        "corpus = ColumnCorpus(data_folder,\n",
        "                      {0 : 'text', 3 : 'ner'},\n",
        "                      train_file='eng.train',\n",
        "                      test_file='eng.testb',\n",
        "                      dev_file='eng.testa')\n",
        "\n",
        "## print statistics of this dataset\n",
        "print(corpus.obtain_statistics())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-16 08:05:19,683 Reading data from conll2003\n",
            "2020-08-16 08:05:19,685 Train: conll2003/eng.train\n",
            "2020-08-16 08:05:19,685 Dev: conll2003/eng.testa\n",
            "2020-08-16 08:05:19,686 Test: conll2003/eng.testb\n",
            "{\n",
            "    \"TRAIN\": {\n",
            "        \"dataset\": \"TRAIN\",\n",
            "        \"total_number_of_documents\": 14987,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 204567,\n",
            "            \"min\": 1,\n",
            "            \"max\": 113,\n",
            "            \"avg\": 13.649629679055181\n",
            "        }\n",
            "    },\n",
            "    \"TEST\": {\n",
            "        \"dataset\": \"TEST\",\n",
            "        \"total_number_of_documents\": 3684,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 46666,\n",
            "            \"min\": 1,\n",
            "            \"max\": 124,\n",
            "            \"avg\": 12.667209554831704\n",
            "        }\n",
            "    },\n",
            "    \"DEV\": {\n",
            "        \"dataset\": \"DEV\",\n",
            "        \"total_number_of_documents\": 3466,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 51578,\n",
            "            \"min\": 1,\n",
            "            \"max\": 109,\n",
            "            \"avg\": 14.881130986728216\n",
            "        }\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqpShyUpSu5G",
        "colab_type": "text"
      },
      "source": [
        "Make tag dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRVilck_Sjec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d998761-90c3-4e95-884d-e016369a7e8d"
      },
      "source": [
        "\n",
        "def make_bert_tag_dict_from_flair_corpus(corpus):\n",
        "    tags_vals = corpus.make_tag_dictionary('ner').get_items()\n",
        "    tags_vals.remove('<unk>')\n",
        "    tags_vals.remove('<START>')\n",
        "    tags_vals.remove('<STOP>')\n",
        "    tags_vals = ['[PAD]'] + tags_vals # + ['X']#, '[CLS]', '[SEP]']\n",
        "    tag2idx = {t : i for i, t in enumerate(tags_vals)}\n",
        "    return tags_vals, tag2idx\n",
        "\n",
        "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
        "\n",
        "print (\"idx2tag:\", idx2tag)\n",
        "print (\"tag2idx:\", tag2idx)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx2tag: ['[PAD]', 'O', 'I-ORG', 'I-MISC', 'I-PER', 'I-LOC', 'B-LOC', 'B-MISC', 'B-ORG']\n",
            "tag2idx: {'[PAD]': 0, 'O': 1, 'I-ORG': 2, 'I-MISC': 3, 'I-PER': 4, 'I-LOC': 5, 'B-LOC': 6, 'B-MISC': 7, 'B-ORG': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_8tmNXJTNB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ce41d2c-deb3-4e38-b25c-6fb69991e3ba"
      },
      "source": [
        "def prepare_flair_corpus(corpus, name='ner', filter_tokens={'-DOCSTART-'}):\n",
        "    result = []\n",
        "    for sent in corpus:\n",
        "        # print (sent)\n",
        "        if sent[0].text in filter_tokens:\n",
        "            continue\n",
        "        else:\n",
        "            result.append(([token.text for token in sent.tokens],\n",
        "                           [token.get_tag(name).value for token in sent.tokens]))\n",
        "            # [token.tags[name].value for token in sent.tokens]))\n",
        "\n",
        "    return result\n",
        "\n",
        "train_dataset = prepare_flair_corpus(corpus.train)\n",
        "val_dataset = prepare_flair_corpus(corpus.dev)\n",
        "print (len(train_dataset), len(val_dataset))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14041 3250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sloTKbCiSyBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aaf4ee25-f2e4-4512-c2dc-60f9ca4f3cec"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "### Due to ordinary use of uppercase for locations and names, we apply bert-base-cased rather than bert-base-uncased.\n",
        "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased',\n",
        "                                              cache_dir=CACHE_DIR,\n",
        "                                              do_lower_case=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "einFkpj4S5TX",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUswZdZgS34z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "fc2998b6-b5f0-4266-b9b4-00418b6398dd"
      },
      "source": [
        "from transformers import BertForTokenClassification\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "class BertForTokenClassificationCustom(BertForTokenClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None, loss_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = (attention_mask.view(-1) == 1)\n",
        "                if loss_mask is not None:\n",
        "                    active_loss &= loss_mask.view(-1)\n",
        "\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # outputs: (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "model = BertForTokenClassificationCustom.from_pretrained('bert-base-cased',\n",
        "                                                         cache_dir=CACHE_DIR,\n",
        "                                                         num_labels=len(tag2idx)).to(device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassificationCustom: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassificationCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassificationCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of BertForTokenClassificationCustom were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR_v4pj8TZN2",
        "colab_type": "text"
      },
      "source": [
        "Optimizer and learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkwhIDsYS_0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "def get_parameters_without_decay(model, no_decay={'bias', 'gamma', 'beta'}):\n",
        "    params_no_decay = []\n",
        "    params_decay = []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any((e in n) for e in no_decay):\n",
        "            params_no_decay.append(p)\n",
        "        else:\n",
        "            params_decay.append(p)\n",
        "\n",
        "    return [{'params': params_no_decay, 'weight_decay': 0.},\n",
        "            {'params': params_decay}]\n",
        "\n",
        "def get_model_parameters(model, no_decay={'bias', 'gamma', 'beta'},\n",
        "                         full_finetuning=True, lr_head=None):\n",
        "    grouped_parameters = get_parameters_without_decay(model.classifier, no_decay)\n",
        "    if lr_head is not None:\n",
        "        for param in grouped_parameters:\n",
        "            param['lr'] = lr_head\n",
        "\n",
        "    if full_finetuning:\n",
        "        grouped_parameters = (get_parameters_without_decay(model.bert, no_decay)\n",
        "                              + grouped_parameters)\n",
        "\n",
        "    return grouped_parameters\n",
        "\n",
        "optimizer = AdamW(get_model_parameters(model),\n",
        "                  lr=LEARNING_RATE, betas=(0.9, 0.999),\n",
        "                  eps =1e-6, weight_decay=0.01, correct_bias=True)#.to(device)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1,\n",
        "                                    num_training_steps=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYL-R4caW4DF",
        "colab_type": "text"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql7RsX2jTeGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "from sklearn.metrics import f1_score as f1_score_sklearn\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "\n",
        "def f1_entity_level(*args, **kwargs):\n",
        "    return f1_score(*args, **kwargs)\n",
        "\n",
        "\n",
        "def f1_token_level(true_labels, predictions):\n",
        "    true_labels = list(itertools.chain(*true_labels))\n",
        "    predictions = list(itertools.chain(*predictions))\n",
        "\n",
        "    labels = list(set(true_labels) - {'[PAD]', 'O'})\n",
        "\n",
        "    return f1_score_sklearn(true_labels,\n",
        "                            predictions,\n",
        "                            average='micro',\n",
        "                            labels=labels)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOsW1VlbYWsS",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stp-wPCJXEEG",
        "colab_type": "text"
      },
      "source": [
        "This set of codes look a little complex. The main function of them is convert the word to the tensor bert model can handle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JQua39yW7tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def bpe_tokenize(words):\n",
        "    new_words = []\n",
        "    bpe_masks = []\n",
        "    for word in words:\n",
        "        bpe_tokens = bpe_tokenizer.tokenize(word)\n",
        "        new_words += bpe_tokens\n",
        "        bpe_masks += [1] + [0] * (len(bpe_tokens) - 1)\n",
        "\n",
        "    return new_words, bpe_masks\n",
        "\n",
        "def prepare_bpe_tokens_for_bert(tokens, max_len):\n",
        "    return [['[CLS]'] + list(toks[:max_len - 2]) + ['[SEP]'] for toks in tokens]\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def create_tensors_for_tokens(bpe_tokenizer, sents, max_len):\n",
        "    return pad_sequences([bpe_tokenizer.convert_tokens_to_ids(sent) for sent in sents],\n",
        "                         maxlen=max_len, dtype='long',\n",
        "                         truncating='post', padding='post')\n",
        "import numpy as np\n",
        "def generate_masks(input_ids):\n",
        "    res = input_ids > 0\n",
        "    return res.astype('float') if type(input_ids) is np.ndarray else res\n",
        "\n",
        "def make_tokens_tensors(tokens, max_len):\n",
        "    bpe_tokens, bpe_masks = tuple(zip(*[bpe_tokenize(sent) for sent in tokens]))\n",
        "    bpe_tokens = prepare_bpe_tokens_for_bert(bpe_tokens, max_len=max_len)\n",
        "    bpe_masks = [[1] + masks[:max_len - 2] + [1] for masks in bpe_masks]\n",
        "    max_len = max(len(sent) for sent in bpe_tokens)\n",
        "    token_ids = torch.tensor(create_tensors_for_tokens(bpe_tokenizer, bpe_tokens, max_len=max_len))\n",
        "    token_masks = generate_masks(token_ids)\n",
        "    return bpe_tokens, max_len, token_ids, token_masks, bpe_masks\n",
        "\n",
        "def add_x_labels(labels, bpe_masks):\n",
        "    result_labels = []\n",
        "    for l_sent, m_sent in zip(labels, bpe_masks):\n",
        "        m_sent = m_sent[1:-1]\n",
        "        sent_res = []\n",
        "        i = 0\n",
        "        for l in l_sent:\n",
        "            sent_res.append(l)\n",
        "\n",
        "            i += 1\n",
        "            while i < len(m_sent) and (m_sent[i] == 0):\n",
        "                i += 1\n",
        "                sent_res.append('[PAD]')\n",
        "\n",
        "        result_labels.append(sent_res)\n",
        "\n",
        "    return result_labels\n",
        "\n",
        "def prepare_bpe_labels_for_bert(labels, max_len):\n",
        "    return [['[PAD]'] + list(ls[:max_len - 2]) + ['[PAD]'] for ls in labels]\n",
        "\n",
        "def create_tensors_for_labels(tag2idx, labels, max_len):\n",
        "    return pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                         maxlen=max_len, value=tag2idx['[PAD]'], padding='post',\n",
        "                         dtype='long', truncating='post')\n",
        "\n",
        "def make_label_tensors(labels, bpe_masks, max_len):\n",
        "    bpe_labels = add_x_labels(labels, bpe_masks)\n",
        "    bpe_labels = prepare_bpe_labels_for_bert(bpe_labels, max_len=max_len)\n",
        "    label_ids = torch.tensor(create_tensors_for_labels(tag2idx, bpe_labels, max_len=max_len))\n",
        "    loss_masks = label_ids != tag2idx['[PAD]']\n",
        "    return label_ids, loss_masks\n",
        "\n",
        "def generate_tensors_for_training(tokens, labels):\n",
        "    _, max_len, token_ids, token_masks, bpe_masks = make_tokens_tensors(tokens, MAX_LEN)\n",
        "    label_ids, loss_masks = make_label_tensors(labels, bpe_masks, max_len)\n",
        "    return token_ids, token_masks, label_ids, loss_masks\n",
        "\n",
        "def make_tensors(dataset_row):\n",
        "    tokens, labels = tuple(zip(*dataset_row))\n",
        "    return generate_tensors_for_training(tokens, labels)\n",
        "\n",
        "def generate_tensors_for_prediction(evaluate, dataset_row):\n",
        "    dataset_row = dataset_row\n",
        "    labels = None\n",
        "    if evaluate:\n",
        "        tokens, labels = tuple(zip(*dataset_row))\n",
        "    else:\n",
        "        tokens = dataset_row\n",
        "\n",
        "    _, max_len, token_ids, token_masks, bpe_masks = make_tokens_tensors(tokens, MAX_LEN)\n",
        "    label_ids = None\n",
        "    loss_masks = None\n",
        "\n",
        "    if evaluate:\n",
        "        label_ids, loss_masks = make_label_tensors(labels, bpe_masks, max_len)\n",
        "\n",
        "    return token_ids, token_masks, bpe_masks, label_ids, loss_masks, tokens, labels\n",
        "\n",
        "def logits_to_preds(logits, bpe_masks, tokens):\n",
        "    preds = logits.argmax(dim=2).numpy()\n",
        "    probs = logits.numpy().max(axis=2)\n",
        "    prob = [np.mean([p for p, m in zip(prob[:len(masks)], masks[:len(prob)]) if m][1:-1])\n",
        "            for prob, masks in zip(probs, bpe_masks)]\n",
        "    preds = [[idx2tag[p] for p, m in zip(pred[:len(masks)], masks[:len(pred)]) if m][1:-1]\n",
        "             for pred, masks in zip(preds, bpe_masks)]\n",
        "    preds = [pred + ['O'] * (max(0, len(toks) - len(pred))) for pred, toks in zip(preds, tokens)]\n",
        "    return preds, prob"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8m3JCjFYLsr",
        "colab_type": "text"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B4Rz00HXcH-",
        "colab_type": "text"
      },
      "source": [
        "This is for evaluation and prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvCfUeRxXXXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model,\n",
        "            dataset,\n",
        "            evaluate=False,\n",
        "            metrics=None,\n",
        "            pred_loader_args={'num_workers' : 1},\n",
        "            pred_batch_size=100):\n",
        "    if metrics is None:\n",
        "        metrics = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            collate_fn=lambda dataset_row: generate_tensors_for_prediction(evaluate, dataset_row),\n",
        "                            **pred_loader_args,\n",
        "                            batch_size=pred_batch_size)\n",
        "\n",
        "    predictions = []\n",
        "    probas = []\n",
        "\n",
        "    if evaluate:\n",
        "        cum_loss = 0.\n",
        "        true_labels = []\n",
        "\n",
        "    for nb, tensors in enumerate(dataloader):\n",
        "        token_ids, token_masks, bpe_masks, label_ids, loss_masks, tokens, labels = tensors\n",
        "\n",
        "        if evaluate:\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            token_ids = token_ids.cuda()\n",
        "            token_masks = token_masks.cuda()\n",
        "\n",
        "            if evaluate:\n",
        "                label_ids = label_ids.cuda()\n",
        "                loss_masks = loss_masks.cuda()\n",
        "\n",
        "            logits = model(token_ids,\n",
        "                          token_type_ids=None,\n",
        "                          attention_mask=token_masks,\n",
        "                          labels=label_ids,\n",
        "                          loss_mask=loss_masks)\n",
        "\n",
        "            if evaluate:\n",
        "                loss, logits = logits\n",
        "                cum_loss += loss.mean().item()\n",
        "            else:\n",
        "                logits = logits[0]\n",
        "\n",
        "            b_preds, b_prob = logits_to_preds(logits.cpu(), bpe_masks, tokens)\n",
        "\n",
        "        predictions.extend(b_preds)\n",
        "        probas.extend(b_prob)\n",
        "\n",
        "    if evaluate:\n",
        "        cum_loss /= (nb + 1)\n",
        "\n",
        "        result_metrics = []\n",
        "        for metric in metrics:\n",
        "            result_metrics.append(metric(true_labels, predictions))\n",
        "\n",
        "        return predictions, probas, tuple([cum_loss] + result_metrics)\n",
        "    else:\n",
        "        return predictions, probas"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfSQLJ2BXkk7",
        "colab_type": "text"
      },
      "source": [
        "The code for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWCBWQm0Xgk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "def train(model,\n",
        "          train_dataset,\n",
        "          val_dataset,\n",
        "          optimizer,\n",
        "          lr_scheduler,\n",
        "          epochs,\n",
        "          batch_size,\n",
        "          validation_metrics = None,\n",
        "          max_grad_norm=1.0,\n",
        "          update_scheduler='es',\n",
        "          smallest_lr=0.,\n",
        "          restore_bm_on_lr_change=False,\n",
        "          decision_metric=None,\n",
        "          keep_best_model=True):\n",
        "    best_model = {}\n",
        "    best_dec_metric = float('inf')\n",
        "\n",
        "    if decision_metric is None:\n",
        "        decision_metric = lambda metrics: metrics[0]\n",
        "\n",
        "    get_lr = lambda: optimizer.param_groups[0]['lr']\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  collate_fn=make_tensors)\n",
        "    from tqdm import trange\n",
        "    iterator = trange(epochs, desc='Epoch')\n",
        "    for epoch in iterator:\n",
        "        model.train()\n",
        "\n",
        "        cum_loss = 0.\n",
        "        for nb, tensors in enumerate(train_dataloader):\n",
        "            token_ids, token_masks, label_ids, loss_masks = tensors\n",
        "            token_ids = token_ids.to(device)\n",
        "            token_masks = token_masks.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "            loss_masks = loss_masks.to(device)\n",
        "\n",
        "            output = model(token_ids,\n",
        "                          token_type_ids=None,\n",
        "                          attention_mask=token_masks,\n",
        "                          labels=label_ids,\n",
        "                          loss_mask=loss_masks)\n",
        "            loss = output[0]\n",
        "            loss = loss.mean()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            if max_grad_norm > 0.:\n",
        "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(),\n",
        "                                               max_norm=max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if update_scheduler == 'es':\n",
        "                lr_scheduler.step()\n",
        "\n",
        "        prev_lr = get_lr()\n",
        "\n",
        "        logger.info(f'Current learning rate: {prev_lr}')\n",
        "\n",
        "        cum_loss /= (nb + 1)\n",
        "        logger.info(f'Train loss: {cum_loss}')\n",
        "\n",
        "        dec_metric = 0.\n",
        "        if val_dataset is not None:\n",
        "            _, __, val_metrics = predict(model, val_dataset, evaluate=True,\n",
        "                                                     metrics=validation_metrics)\n",
        "\n",
        "            val_loss = val_metrics[0]\n",
        "            logger.info(f'Validation loss: {val_loss}')\n",
        "            logger.info(f'Validation metrics: {val_metrics[1:]}')\n",
        "\n",
        "            dec_metric = decision_metric(val_metrics)\n",
        "\n",
        "            if keep_best_model and (dec_metric < best_dec_metric):\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "                best_dec_metric = dec_metric\n",
        "\n",
        "        if restore_bm_on_lr_change and get_lr() < prev_lr:\n",
        "            if get_lr() < smallest_lr:\n",
        "                iterator.close()\n",
        "                break\n",
        "\n",
        "            prev_lr = get_lr()\n",
        "            logger.info(f'Reduced learning rate to: {prev_lr}')\n",
        "\n",
        "            logger.info('Restoring best model...')\n",
        "            model.load_state_dict(best_model)\n",
        "    if best_model:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGtjTBDYRzL",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9WkznW8Xttp",
        "colab_type": "text"
      },
      "source": [
        "Run the training code. It will take around 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_muUoB9EXrMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "634eff7e-82f0-40b3-e39c-2319f32d2d4b"
      },
      "source": [
        "train(model,\n",
        "      train_dataset,\n",
        "      val_dataset,\n",
        "      optimizer,\n",
        "      lr_scheduler,\n",
        "      epochs=MAX_N_EPOCHS,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      validation_metrics = [f1_entity_level],\n",
        "      max_grad_norm=1.0,\n",
        "      update_scheduler='es',\n",
        "      smallest_lr=0.,\n",
        "      restore_bm_on_lr_change=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:sequence_tagger_bert:Current learning rate: 2.656777568696534e-05\n",
            "INFO:sequence_tagger_bert:Train loss: 0.08158245983208447\n",
            "INFO:sequence_tagger_bert:Validation loss: 0.03414060947816876\n",
            "INFO:sequence_tagger_bert:Validation metrics: (0.9400234545149941,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [02:46<02:46, 166.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:sequence_tagger_bert:Current learning rate: 3.132882251671538e-06\n",
            "INFO:sequence_tagger_bert:Train loss: 0.01903606630968628\n",
            "INFO:sequence_tagger_bert:Validation loss: 0.02773769273577879\n",
            "INFO:sequence_tagger_bert:Validation metrics: (0.9502688172043011,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 2/2 [05:31<00:00, 165.69s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J9OdPUqX4lN",
        "colab_type": "text"
      },
      "source": [
        "### Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87VhO0amX1OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de877de0-c724-4d17-971f-4aedef3ab157"
      },
      "source": [
        "test_dataset = prepare_flair_corpus(corpus.test)\n",
        "_, __, test_metrics = predict(model, test_dataset, evaluate=True,\n",
        "                                         metrics=[f1_entity_level, f1_token_level])\n",
        "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
        "logger.info(f'Token-level f1: {test_metrics[2]}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sequence_tagger_bert:Entity-level f1: 0.9091069226704246\n",
            "INFO:sequence_tagger_bert:Token-level f1: 0.9269188395152406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnD6c9qrX-Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Two Prediction Examples"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to34EwFeYC5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fa6e315a-5c2a-42ad-c456-bda73cda2a38"
      },
      "source": [
        "predict(model, [['We', 'are', 'living', 'in', 'Singapore', '.'],\n",
        "                    ['Prof', 'Bingtian', 'Dai', 'enjoys', 'his', 'classes', '.']])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['O', 'O', 'O', 'O', 'I-LOC', 'O'],\n",
              "  ['O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O']],\n",
              " [9.81316, 8.509908])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx3Z-XWgZU6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}